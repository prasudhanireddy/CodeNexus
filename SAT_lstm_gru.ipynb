{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9e0f14f-7add-4beb-b51c-a22cb530390a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Vedhanshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3715, 67, 199, 8, 420, 22, 29, 2, 19, 327, 5, 3084, 9, 923, 5, 409, 2, 105, 41, 1, 14, 7, 47, 32, 3, 679, 5962, 211, 22, 679, 2, 449, 15, 1133, 11, 4, 2613, 107, 6, 726, 1, 336, 186, 315, 2902, 15, 217, 3, 9326, 527, 350, 2, 3716, 21, 4, 3441, 40, 2822, 68, 7703, 14, 9, 35, 540, 3, 94, 257, 17, 7703, 1152, 12968, 336, 5, 1325, 40, 861, 1022, 94, 257, 17, 120, 337, 168, 71, 7703, 19, 1, 4106, 8, 4612, 21, 140, 336, 309, 7, 13, 519, 11, 2, 46, 105, 108, 12, 14, 5, 345, 168, 15, 758, 53, 20, 197, 11, 747, 7, 52, 35, 205, 350, 406, 3715, 106, 99, 1296, 1196, 6, 68, 151, 199, 3, 67, 250, 12969, 21, 9, 7704, 220, 4107, 14, 2, 327, 2, 83, 279, 4, 27, 8, 9, 329, 5, 2384, 4325, 2, 37, 99, 5963, 16, 2, 41, 12, 6, 1, 1758, 1555, 181, 2, 229, 16, 4, 1804, 8, 14, 11, 562, 5, 171, 230, 2, 656, 103, 2, 25, 250, 18, 25, 4, 2549, 61, 440, 12, 3, 12970, 5964, 12971, 444, 1296, 29, 242, 1133, 4, 32, 47, 211, 2, 299, 17, 1, 3899, 917, 12972, 37, 318, 8, 4, 2823, 188, 113, 1, 243, 257, 12, 19, 4, 32, 188, 282, 3, 1326, 7705, 163, 5, 574, 715, 8, 181, 49, 4613, 16, 1118, 796, 12, 90, 679, 9, 355, 275, 55, 28, 144, 2253, 3, 1, 41, 520, 4, 214, 2, 1074, 10, 466, 124, 5396, 20, 7, 13, 30, 163, 5, 889, 16, 6, 1403, 1142, 8, 9327, 826, 14, 2, 35, 85, 3, 2, 41, 9, 1296, 14, 270, 24, 3442, 270, 20, 5397, 2, 25, 92, 4, 11, 1, 4107, 6701, 7706, 40, 9, 1013, 211, 2, 25, 635, 17, 2, 37, 5, 34, 205, 5, 4, 11, 12, 5, 57, 1, 113, 2, 25, 118, 6, 139, 12, 520, 1363, 964, 2, 1614, 518, 10, 5, 460, 4, 132, 20, 26, 8, 100, 7, 13, 122, 3, 270, 1, 4949, 83, 574, 4, 137, 1165, 8, 47, 3, 9328, 2, 1297, 17, 1, 158, 193, 5, 119, 4, 1296, 29, 15, 5, 78, 239, 18, 16, 11, 562, 3443, 4, 5398, 21, 17, 29, 3, 178, 6, 117, 736, 3203, 156, 18, 12973, 22, 15, 240, 325, 75, 329, 14, 83, 5399, 107, 182, 2, 62, 12, 2, 25, 632, 3, 104, 4325, 67, 250, 12, 2, 12974, 104, 8, 1, 4325, 17, 2, 3900, 16, 182, 3, 1575, 719, 61, 10, 97, 100, 9329, 1404, 4325, 3, 777, 4, 97, 973, 5, 96, 12, 14, 19, 4, 1153, 12975, 101, 1046, 32, 3444, 6, 2684, 12976, 4325, 3, 163, 1197, 5, 590, 16, 1118, 796, 4326, 1, 188, 15, 487, 163, 5, 5400, 212, 2756, 2614, 3, 1, 144, 87, 4, 7707, 679, 101, 83, 9330, 796, 9, 12977, 87, 3570, 3571, 918, 21, 12, 3, 2, 64, 5401, 61, 680, 7708, 215, 371, 100, 1, 282, 87, 104, 1276, 68, 12978, 336, 16, 1, 224, 3, 2, 64, 2757, 5, 93, 12979, 28, 261, 2, 477, 37, 5, 80, 332, 5, 12, 5, 1593, 10, 56, 2985, 26, 21, 117, 29, 33, 221, 5, 500, 10, 662, 53, 67, 486, 2903, 382, 56, 2986, 2030, 252, 3085, 2, 19, 99, 1296, 68, 213, 4, 289, 11, 12, 6, 238, 358, 100, 3, 7, 77, 548, 4, 2496, 8, 35, 2, 1075, 309, 2, 3901, 1114, 2, 1783, 5965, 7709, 1296, 87, 18, 44, 32, 12980, 12981, 12982, 3, 729, 6702, 4, 132, 1184, 7, 376, 594, 5, 35, 20, 383, 178, 7, 273, 55, 12983, 2, 46, 202, 22, 15, 4, 32, 29, 6, 1196, 1296, 325, 24, 9331, 1198, 117, 414, 917, 328, 2, 192, 2385, 2031, 93, 1381, 211, 2, 46, 1075, 17, 18, 46, 1805, 45, 324, 16, 414, 917, 26, 18, 269, 16, 1225, 266, 1196, 12984, 13, 1806, 53, 8], [12, 52, 233, 33, 387, 2, 31, 12, 14, 70, 7, 316, 527, 222, 11, 1, 470, 3, 811, 7, 876, 45, 4, 27, 8, 14, 2, 249, 7, 46, 36, 951, 30, 51, 24, 7710, 5, 35, 1716, 1036, 12, 14, 52, 611, 75, 355, 7, 183, 4, 79, 170, 28, 2, 748, 250, 40, 348, 4, 225, 34, 280, 51, 33, 221, 5, 80, 332, 1556, 1466, 3, 118, 215, 80, 24, 71, 12, 14, 2, 607, 6, 318, 60, 782, 182, 7, 393, 3445, 3, 4950, 9, 355, 7, 90, 13, 4, 79, 270, 16, 1, 244, 28, 51, 33, 19, 4, 137, 86, 12, 228, 13, 24, 6, 33, 2, 19, 5, 1382, 7, 13, 3902, 3, 222, 20, 7, 13, 24, 227, 1, 387], [69, 22, 2211, 307, 603, 7711, 12985, 13, 45, 28, 422]]\n",
      "Sample input size:  torch.Size([64, 200])\n",
      "Sample input: \n",
      " tensor([[   0,    0,    0,  ...,  419,   29,  990],\n",
      "        [   0,    0,    0,  ...,   21,   22,  133],\n",
      "        [   0,    0,    0,  ...,    4,   32, 1467],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,   43,  166,  451],\n",
      "        [   0,    0,    0,  ...,   30,    3,  308],\n",
      "        [   0,    0,    0,  ...,  605,    3,  181]], dtype=torch.int32)\n",
      "\n",
      "Sample label size:  torch.Size([64])\n",
      "Sample label: \n",
      " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "df=pd.read_csv('review1.csv') # name of the csv file is 'review.csv'\n",
    "\n",
    "text = df['text'] #extracting the reviews\n",
    "label = df['score'] #extracting the ratings\n",
    "\n",
    "text[0]\n",
    "\n",
    "len(label)\n",
    "\n",
    "nltk.download('wordnet')  # Download WordNet data\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_text = []\n",
    "for review in text:\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in review.split()]\n",
    "    lemmatized_text.append(\" \".join(lemmatized_words))\n",
    "\n",
    "#Tokenize â€” Create Vocab to Int mapping dictionary\n",
    "from collections import Counter\n",
    "all_text2 = ' '.join(text)\n",
    "# create a list of words\n",
    "words = all_text2.split()\n",
    "# Count all the words using Counter Method\n",
    "count_words = Counter(words)\n",
    "\n",
    "total_words = len(words)\n",
    "sorted_words = count_words.most_common(total_words)\n",
    "\n",
    "#In order to create a vocab to int mapping dictionary\n",
    "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
    "\n",
    "vocab_to_int\n",
    "\n",
    "#encoding of reviews (replace words in our reviews by integers)\n",
    "reviews_int = []\n",
    "for review in text:\n",
    "    r = [vocab_to_int[w] for w in review.split()]\n",
    "    reviews_int.append(r)\n",
    "print (reviews_int[0:3])\n",
    "\n",
    "labels = np.array(label)\n",
    "#converting to binary class\n",
    "l=[]\n",
    "for i in labels:\n",
    "  if(i==0 or i==1 or i==2):\n",
    "    i=0\n",
    "  elif(i==3 or i==4 or i==5):\n",
    "    i=1\n",
    "  l.append(i)\n",
    "\n",
    "label = np.array(l)\n",
    "\n",
    "def pad_input(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features\n",
    "\n",
    "seq_len = 200  # The length that the sentences will be padded/shortened to\n",
    "\n",
    "reviews = pad_input(reviews_int, seq_len)\n",
    "\n",
    "\n",
    "\n",
    "reviews[0]\n",
    "\n",
    "#80% train, 10% test & 10% validation\n",
    "split_frac = 0.8\n",
    "len_feat = len(reviews)\n",
    "train_x = reviews[0:int(split_frac*len_feat)]\n",
    "train_y = label[0:int(split_frac*len_feat)]\n",
    "remaining_x = reviews[int(split_frac*len_feat):]\n",
    "remaining_y = label[int(split_frac*len_feat):]\n",
    "valid_x = remaining_x[0:int(len(remaining_x)*0.5)]\n",
    "valid_y = remaining_y[0:int(len(remaining_y)*0.5)]\n",
    "test_x = remaining_x[int(len(remaining_x)*0.5):]\n",
    "test_y = remaining_y[int(len(remaining_y)*0.5):]\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
    "\n",
    "# obtain one batch of training data\n",
    "for sample_x, sample_y in train_loader:\n",
    "    print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "    print('Sample input: \\n', sample_x)\n",
    "    print()\n",
    "    print('Sample label size: ', sample_y.size()) # batch_size\n",
    "    print('Sample label: \\n', sample_y)\n",
    "    break  # Break after printing the first batch to avoid printing the entire dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ed0fb3c-327f-4d74-846c-7a7ea0503af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 86.29%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        output = self.fc(lstm_out)\n",
    "        output = self.sigmoid(output)\n",
    "        return output.squeeze(1)\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = len(vocab_to_int) + 1\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        predicted = torch.round(outputs)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "accuracy = total_correct / total_samples\n",
    "print('Test Accuracy: {:.2f}%'.format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74f5586b-1e83-4e5d-8709-c7ced75d4af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 90.22%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        gru_out, _ = self.gru(embedded)\n",
    "        gru_out = gru_out[:, -1, :]\n",
    "        output = self.fc(gru_out)\n",
    "        output = self.sigmoid(output)\n",
    "        return output.squeeze(1)\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = len(vocab_to_int) + 1\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "\n",
    "# Instantiate the model\n",
    "model = GRUModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        predicted = torch.round(outputs)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "accuracy = total_correct / total_samples\n",
    "print('Test Accuracy: {:.2f}%'.format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1cb043-36e2-4b79-9e1b-0a6093020bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035d6711-4829-4656-8b56-d84065f4ad3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dd4978-c072-4e32-b045-8b6591fe8dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee9de74-cf09-4f30-90e8-17d7566019bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
